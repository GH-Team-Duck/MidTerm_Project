{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c7bcf6",
   "metadata": {},
   "source": [
    "# Github Topic and Repositories Scrapper\n",
    "    \n",
    "   ### Objective:\n",
    "         - To get the Github page of topics, url : \"www.github.com/topics\"\n",
    "         - Parse the downloaded html content using Beautiful Soup\n",
    "         - Get the desired contents/list of contents from the soup object.\n",
    "         - Make a DataFrame using pandas libraries of the scraped data.\n",
    "         - Finally save the dataframe as .csv or .xlsx according to our preference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437e7c2",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98ff9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903f9e2",
   "metadata": {},
   "source": [
    "### As we are Scraping Github topics page, we are assigning the url to a variable named topics_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95edef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url = 'https://www.github.com/topics'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f7693",
   "metadata": {},
   "source": [
    "**We are sending a get request using the python requests library and we are getting the html page as the response to the request sent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(topics_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7387f",
   "metadata": {},
   "source": [
    "**We are checking the status code to ensure we have successfully got the webpage response. Status code 200 means that the request was successful. To know more about HTTP Status code, you can refer to [MDN References](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) of HTTP response status codes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e52f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe27c64",
   "metadata": {},
   "source": [
    "**Getting the HTML content from the downloaded page.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d7f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagecontent = response.text\n",
    "# len(pagecontent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142b402",
   "metadata": {},
   "source": [
    "**Parsing the HTML Content recieved from the get requests, using the Beautiful Soup Library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(pagecontent,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27f25a",
   "metadata": {},
   "source": [
    "**We can modify the apperance of the HTML content that we parsed, we can use a function called *Prettify()*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62994ea1",
   "metadata": {},
   "source": [
    "**We made a function *parse_star_count(star_str)* that parses the star count(passed as a string argument to the function) into a more readable format.  \n",
    "For example:  \n",
    "80.3K will be written/parsed as 80300**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ebb1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(star_str):\n",
    "    star_str = star_str.strip()\n",
    "    if star_str[-1] == 'k':\n",
    "        return int(float(star_str[:-1])*1000)\n",
    "    return int(star_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b447788",
   "metadata": {},
   "source": [
    "**Utility function _get_repo_info(h3_tag,star_tag)_ that seperates username, reponame, repo_url and stars count of the repository from the h3 tag list and star_tag list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cf82bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_repo_info(h3_tag,star_tag):\n",
    "    atags = h3_tag.find_all('a')\n",
    "    username = atags[0].text.strip()\n",
    "    reponame = atags[1].text.strip()\n",
    "    repo_url = \"https://www.github.com\"+ atags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    \n",
    "    return username,reponame,repo_url,stars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b65d7f",
   "metadata": {},
   "source": [
    "**Function get_topic_page(topic_url) : It takes the topic url as an argument and then fetches the content using the get request of ___requests___ library and then parse the content using BeautifulSoup and returns the same parsed object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea737a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code !=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "        \n",
    "    topic_doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return topic_doc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602177ee",
   "metadata": {},
   "source": [
    "**Function _get_topic_repos(topic_doc)_: It takes a Beautiful Soup Object as an argument and find all the h3 tags that contains information about the User name, Repository Name, Repo Url, Stars, stores them in a dictionary and return a object created by converting the dictionary to a Pandas DataFrame**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8aa0088",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_topic_repos(topic_doc):\n",
    "    \n",
    "    repo_tags = topic_doc.find_all('h3',attrs={\n",
    "        'class' : 'f3 color-fg-muted text-normal lh-condensed'})\n",
    "    stars_tags= topic_doc.find_all('span',attrs= {\n",
    "        'class' : 'Counter js-social-count'\n",
    "            })\n",
    "    topic_repo_dicts = {\n",
    "    'Username':[],\n",
    "    'Repo_Name':[],\n",
    "    'Repo_Url':[],\n",
    "    'Stars': []\n",
    "    }\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i],stars_tags[i])\n",
    "        topic_repo_dicts['Username'].append(repo_info[0])\n",
    "        topic_repo_dicts['Repo_Name'].append(repo_info[1])\n",
    "        topic_repo_dicts['Repo_Url'].append(repo_info[2])\n",
    "        topic_repo_dicts['Stars'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repo_dicts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165823e9",
   "metadata": {},
   "source": [
    "**Function _scrape_topic(topic_url,topic_name)_: It scrapes the Top Repositories from the topic url and saves the scraped data as a dataframe to a .csv file, having the file name of the title.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "806f06bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url,topic_name):\n",
    "    filename = topic_name+\".csv\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File {filename} already exists. Skipping...\")\n",
    "        return \n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    \n",
    "    topic_df.to_csv(filename,index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3169c9",
   "metadata": {},
   "source": [
    "**Function _get_topic_titles(soup)_: It takes a Beautiful Soup object and finds all the topic titles  present in all the paragraph(p) tags of the html page and returns the same's list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "315163d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(soup):\n",
    "    topic_title_tags = soup.find_all('p',attrs={\n",
    "    'class':'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    })\n",
    "    topic_titles = []\n",
    "    \n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e3dcf",
   "metadata": {},
   "source": [
    "**Function _get_topic_description(soup)_: It takes a Beautiful Soup object and finds all the descriptions present in all the paragraph(p) tags of the html page and returns the same's list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485b5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_description(soup):\n",
    "    topic_desc_tag = soup.find_all('p', attrs=\n",
    "                                   {'class':'f5 color-fg-muted mb-0 mt-1'})\n",
    "    topic_descriptions = []\n",
    "\n",
    "    for tag in topic_desc_tag:\n",
    "        topic_descriptions.append(tag.text.strip())\n",
    "    return topic_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de1cf6",
   "metadata": {},
   "source": [
    "**Function _get_topic_urls(soup)_: It takes a Beautiful Soup object and finds all the links present in all the anchor tags of the html page and returns the same's list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd1a3ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(soup):\n",
    "    topic_link_tags = soup.find_all('a',attrs = \n",
    "                            {'class' : 'no-underline flex-grow-0'})\n",
    "    topic_urls = []\n",
    "    base = \"https://www.github.com\"\n",
    "    page =\"?page=1\"\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base+tag['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce4a9b1",
   "metadata": {},
   "source": [
    "**Function _scrape_topics()_: scrapes the topic title from the page and their corresponding description and url and returns a DataFrame object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05a54785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url = \"https://github.com/topics\"\n",
    "#     topics_url = \"https://github.com/topics?page=1\"\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code !=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))  \n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    topics_dict = {\n",
    "        'title' : get_topic_titles(soup),\n",
    "        'decscription': get_topic_description(soup),\n",
    "        'url':get_topic_urls(soup)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433672f",
   "metadata": {},
   "source": [
    "**Bringing all the functions and their activities under one function _scrape_topic_repos()_: Which first scrapes the topic title from the page and their corresponding description and url, using the function _scrape_topics()_ and then using the url scrapes the top repositories of that particular topic.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89ae1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print(\"Scraping list topics from Github\")\n",
    "    topics_df = scrape_topics()\n",
    "    for index,row in topics_df.iterrows():\n",
    "        print(f\"Scraping top repositories for {row['title']}\")\n",
    "        scrape_topic(row['url'],row['title'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb758d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_topics_repos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
